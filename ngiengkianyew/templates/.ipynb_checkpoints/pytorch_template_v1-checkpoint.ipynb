{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3a8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/kianyewngieng/PycharmProjects/my_learning/')\n",
    "\n",
    "import pickle\n",
    "with open('train_model_input.pickle', 'rb') as stream:\n",
    "    train_model_input = pickle.load(stream)\n",
    "with open('test_model_input.pickle', 'rb') as stream:\n",
    "    test_model_input = pickle.load(stream)\n",
    "with open('feature_max_idx.pickle', 'rb') as stream:\n",
    "    feature_max_idx = pickle.load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e59fc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>hist_movie_id</th>\n",
       "      <th>hist_genres</th>\n",
       "      <th>hist_len</th>\n",
       "      <th>genres</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "      <th>zip</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5561</td>\n",
       "      <td>368</td>\n",
       "      <td>[497, 170, 209, 2362, 70, 538, 2252, 217, 111,...</td>\n",
       "      <td>[1, 1, 4, 8, 8, 8, 8, 1, 3, 8, 1, 8, 1, 8, 7, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4530</td>\n",
       "      <td>3681</td>\n",
       "      <td>[399, 684, 103, 29, 27, 2879, 1081, 1430, 2971...</td>\n",
       "      <td>[5, 5, 3, 3, 6, 8, 13, 5, 8, 12, 8, 5, 5, 5, 3...</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2913</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2895</td>\n",
       "      <td>1487</td>\n",
       "      <td>[30, 48, 187, 136, 125, 341, 118, 212, 406, 43...</td>\n",
       "      <td>[1, 1, 5, 8, 15, 5, 8, 5, 5, 8, 1, 10, 5, 1, 1...</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4082</td>\n",
       "      <td>684</td>\n",
       "      <td>[582, 1512, 876, 216, 291, 215, 167, 1614, 282...</td>\n",
       "      <td>[5, 8, 14, 5, 5, 1, 5, 5, 1, 11, 8, 5, 8, 8, 5...</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21</td>\n",
       "      <td>2515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3294</td>\n",
       "      <td>3036</td>\n",
       "      <td>[28, 69, 1177, 158, 260, 2240, 55, 145, 48, 50...</td>\n",
       "      <td>[1, 5, 5, 5, 11, 8, 1, 8, 1, 8, 5, 3, 5, 5, 5,...</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2497</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id                                      hist_movie_id  \\\n",
       "0     5561       368  [497, 170, 209, 2362, 70, 538, 2252, 217, 111,...   \n",
       "1     4530      3681  [399, 684, 103, 29, 27, 2879, 1081, 1430, 2971...   \n",
       "2     2895      1487  [30, 48, 187, 136, 125, 341, 118, 212, 406, 43...   \n",
       "3     4082       684  [582, 1512, 876, 216, 291, 215, 167, 1614, 282...   \n",
       "4     3294      3036  [28, 69, 1177, 158, 260, 2240, 55, 145, 48, 50...   \n",
       "\n",
       "                                         hist_genres  hist_len  genres  \\\n",
       "0  [1, 1, 4, 8, 8, 8, 8, 1, 3, 8, 1, 8, 1, 8, 7, ...        20       1   \n",
       "1  [5, 5, 3, 3, 6, 8, 13, 5, 8, 12, 8, 5, 5, 5, 3...        20       7   \n",
       "2  [1, 1, 5, 8, 15, 5, 8, 5, 5, 8, 1, 10, 5, 1, 1...        20       8   \n",
       "3  [5, 8, 14, 5, 5, 1, 5, 5, 1, 11, 8, 5, 8, 8, 5...        20       5   \n",
       "4  [1, 5, 5, 5, 11, 8, 1, 8, 1, 8, 5, 3, 5, 5, 5,...        20       6   \n",
       "\n",
       "   gender  age  occupation   zip  label  \n",
       "0       1  3.0           4  2311      0  \n",
       "1       1  4.0           2  2913      0  \n",
       "2       2  3.0          12   309      0  \n",
       "3       2  3.0          21  2515      0  \n",
       "4       2  2.0           5  2497      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "train_pd = pd.DataFrame.from_dict(train_model_input, orient='index').T.applymap(lambda x: x.detach().numpy())\n",
    "test_pd = pd.DataFrame.from_dict(test_model_input, orient='index').T.applymap(lambda x: x.detach().numpy())\n",
    "\n",
    "data_pd = pd.DataFrame.from_dict(train_model_input,orient='index').T.applymap(lambda x: x.detach().numpy())\n",
    "data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44314376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import defaultdict, OrderedDict\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import itertools\n",
    "\n",
    "DEFAULT_GROUP_NAME = 'default_group'\n",
    "\n",
    "@dataclass\n",
    "class SparseFeat:\n",
    "    \"\"\" Class to store metadata related to a categorical feature,\n",
    "    that will be converted to pytorch tensor via torch.nn.Embedding()\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    vocabulary_size: int\n",
    "    embedding_dim: int\n",
    "    embedding_name: str = None\n",
    "    group_name: str = DEFAULT_GROUP_NAME\n",
    "    dtype: str = torch.long\n",
    "            \n",
    "    def __post_init__(self):\n",
    "        if self.embedding_name is None:\n",
    "            self.embedding_name = self.name\n",
    "        \n",
    "@dataclass\n",
    "class VarLenSparseFeat:\n",
    "    \"\"\"Class to store metadata related to a variable length feature (i.e. list)\n",
    "    - Note: Must create a new SparseFeat with different name, if the feature is a variable length feature of a categorical column. Cannot reuse the instantiated sparse_feat because the embeddings are created separately.\n",
    "    \"\"\"\n",
    "    sparse_feat: SparseFeat\n",
    "    max_len: int\n",
    "    combiner: str = 'mean'\n",
    "    length_name: str = None\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.sparse_feat.name\n",
    "    \n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return self.sparse_feat.vocabulary_size\n",
    "    \n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.sparse_feat.embedding_dim\n",
    "    \n",
    "    @property\n",
    "    def embedding_name(self):\n",
    "        return self.sparse_feat.embedding_name\n",
    "    \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.sparse_feat.dtype\n",
    "    \n",
    "    @property\n",
    "    def group_name(self):\n",
    "        return self.sparse_feat.group_name\n",
    "    \n",
    "@dataclass\n",
    "class DenseFeat:\n",
    "    name: str\n",
    "    dimension: int\n",
    "    dtype: str = None\n",
    "        \n",
    "def build_feature_positions(feature_columns):\n",
    "    \"\"\"Based on the ordering of the feature columns, get the position index\"\"\"\n",
    "    feature_positions = OrderedDict()\n",
    "    start = 0\n",
    "    for feat in feature_columns:\n",
    "        feat_name = feat.name\n",
    "        if isinstance(feat, SparseFeat):\n",
    "            feature_positions[feat_name] = (start, start+1)\n",
    "            start += 1\n",
    "            \n",
    "        elif isinstance(feat, DenseFeat):\n",
    "            feature_positions[feat_name] = (start, start + feat.dimension)\n",
    "            start += feat.dimension\n",
    "            \n",
    "        elif isinstance(feat, VarLenSparseFeat):\n",
    "            feature_positions[feat_name] = (start, start + feat.max_len)\n",
    "            start += feat.max_len\n",
    "    \n",
    "            if feat.length_name not in feature_positions and feat.length_name is not None:\n",
    "                feature_positions[feat.length_name] = (start, start+1)\n",
    "                start +=1 \n",
    "        else:\n",
    "            raise TypeError('Invalid feature columns type, got', type(feat))\n",
    "    return feature_positions\n",
    "\n",
    "def build_pytorch_dataset(df_pd, feature_columns):\n",
    "    torch_df = {}\n",
    "    for feat in feature_columns:\n",
    "        feat_name = feat.name\n",
    "        if isinstance(feat, SparseFeat) or isinstance(feat, DenseFeat):\n",
    "            input_tensor = torch.tensor(df_pd[feat_name].values).reshape(-1,1)\n",
    "            torch_df[feat_name] = input_tensor\n",
    "            \n",
    "        elif isinstance(feat, VarLenSparseFeat):\n",
    "            input_tensor = torch.stack(list(map(lambda x: torch.tensor(x[:feat.max_len]), df_pd[feat_name].values)))\n",
    "            torch_df[feat_name] = input_tensor\n",
    "            \n",
    "            if feat.length_name is not None and feat.length_name not in torch_df:\n",
    "                torch_df[feat.length_name] = torch.tensor(df_pd[feat.length_name].values).reshape(-1,1)\n",
    "        else:\n",
    "            raise TypeError('Invalid feature columns type, got,', type(feat))\n",
    "    torch_df = torch.cat(list(torch_df.values()), dim=-1)\n",
    "    return torch_df\n",
    "\n",
    "\n",
    "def build_embedding_dict(all_sparse_feature_columns, init_std=0.001, device='cpu'):\n",
    "    \"\"\" Returns a dictionary with key as the feat.name and value of the instantiated nn.Embedding\n",
    "    \"\"\"\n",
    "    embedding_dict = nn.ModuleDict(\n",
    "    {feat.name: nn.Embedding(feat.vocabulary_size,\n",
    "                            feat.embedding_dim) for feat in all_sparse_feature_columns})\n",
    "    if init_std is not None:\n",
    "        for tensor in embedding_dict.values():\n",
    "            # nn.init is in_place\n",
    "            nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "            \n",
    "    return embedding_dict.to(device)\n",
    "\n",
    "class TensorLookup:\n",
    "    \"\"\"Wrapper for a torch.tensor dataframe to allow selection of data according to the corresponding column name\n",
    "    Args:\n",
    "        X (torch.Tensor): torch tensor that is created from\n",
    "    \"\"\"\n",
    "    def __init__(self, X, feature_positions):\n",
    "        self.X = X\n",
    "        self.feature_positions = feature_positions\n",
    "        \n",
    "    def __getitem__(self, feature_names):\n",
    "        if type(feature_names) == str:\n",
    "            idx_lookup = self.feature_positions[feature_names]\n",
    "            return self.X[:, idx_lookup[0]:idx_lookup[1]]\n",
    "        \n",
    "        if type(feature_names) == list:\n",
    "            features_tuple = (\n",
    "                self.X[:,self.feature_positions[feat_name][0]: \n",
    "                       self.feature_positions[feat_name][1]] for feat_name in feature_names\n",
    "            )\n",
    "            return tuple(features_tuple)\n",
    "            \n",
    "def embedding_lookup(X, \n",
    "                     feature_positions,\n",
    "                     embedding_dict,\n",
    "                     sparse_feature_columns,\n",
    "                     return_feat_list=(),\n",
    "                     mask_feat_list=(),\n",
    "                     to_list=False):\n",
    "    \"\"\"returns a dictionary with key as the group name, value as the list of embedding output of shape (B,1, E)\n",
    "    if specify to list, then we get a list of embedding output of shape (B,1, E)\n",
    "    \"\"\"\n",
    "    group_embedding_dict = defaultdict(list)\n",
    "    for feat in sparse_feature_columns:\n",
    "        feat_name = feat.name\n",
    "        embedding_name = feat.embedding_name\n",
    "        if feat_name in return_feat_list or len(return_feat_list)==0:\n",
    "            lookup_idx = feature_positions[feat_name]\n",
    "            input_tensor =  X[:, lookup_idx[0]:lookup_idx[1]].long()\n",
    "            embedding = embedding_dict[embedding_name](input_tensor)\n",
    "            group_embedding_dict[feat.group_name].append(embedding)\n",
    "    if to_list == True:\n",
    "        return list(itertools.chain.from_iterable(group_embedding_dict.values()))\n",
    "    return group_embedding_dict\n",
    "\n",
    "\n",
    "def varlen_embedding_lookup(X, \n",
    "                     feature_positions,\n",
    "                     embedding_dict,\n",
    "                     varlen_feature_columns):\n",
    "    \"\"\"Returns a dictionary with key as the feat.name and value as the embedding output of shape (B,T,E)\"\"\"\n",
    "    varlen_group_embedding_dict = {}\n",
    "    for feat in varlen_feature_columns:\n",
    "        feat_name = feat.name\n",
    "        embedding_name = feat.embedding_name\n",
    "        lookup_idx = feature_positions[feat_name]\n",
    "        input_tensor =  X[:, lookup_idx[0]:lookup_idx[1]].long()\n",
    "        embedding = embedding_dict[embedding_name](input_tensor)\n",
    "        varlen_group_embedding_dict[feat_name] = embedding\n",
    "    return varlen_group_embedding_dict\n",
    "\n",
    "def varlen_embedding_pooled_lookup(X,\n",
    "                                   feature_positions,\n",
    "                                  embedding_dict,\n",
    "                                  varlen_sparse_feature_columns,\n",
    "                                   return_feat_list=(),\n",
    "                                   to_list=False):\n",
    "    \"\"\"returns a dictionary with with key as the group name, \n",
    "    value as the list of embedding output of shape (B,1, E)\"\"\"\n",
    "    group_embedding_dict = defaultdict(list)\n",
    "    for feat in varlen_sparse_feature_columns:\n",
    "        feat_name = feat.name\n",
    "        embedding_name = feat.embedding_name\n",
    "        if feat_name in return_feat_list or len(return_feat_list) == 0:\n",
    "            lookup_idx = feature_positions[feat_name]\n",
    "            input_tensor = X[:, lookup_idx[0]:lookup_idx[1]].long()\n",
    "            seq_embedding = embedding_dict[embedding_name](input_tensor)\n",
    "            \n",
    "            if feat.length_name is None:\n",
    "                lookup_idx = feature_positions[feat_name]\n",
    "                seq_mask = X[:, lookup_idx[0]:lookup_idx[1]].long() != 0 # (B,T)\n",
    "                emb = SequencePoolingLayer(mode=feat.combiner, supports_masking=True, device=seq_embedding.device)(seq_embedding, seq_mask)\n",
    "            else:\n",
    "                lookup_idx = feature_positions[feat.length_name]\n",
    "                seq_length = X[:, lookup_idx[0]:lookup_idx[1]].long() # (B, 1)\n",
    "                emb = SequentialPoolingLayer(mode=feat.combiner, supports_masking=False, device=seq_embedding.device)(seq_embedding, seq_length)\n",
    "                \n",
    "            group_embedding_dict[feat.group_name].append(emb)\n",
    "    if to_list:\n",
    "        return list(itertools.chain.from_iterable(group_embedding_dict.values()))\n",
    "    return group_embedding_dict\n",
    "\n",
    "def dense_lookup(X, feature_positions, dense_feat_columns):\n",
    "    dense_out = []\n",
    "    for feat in dense_feat_columns:\n",
    "        feat_name = feat.name\n",
    "        lookup_idx = feature_positions[feat_name]\n",
    "        dense_out.append(X[:, lookup_idx[0]:lookup_idx[1]])\n",
    "    return dense_out\n",
    "\n",
    "class SequentialPoolingLayer(nn.Module):\n",
    "    def __init__(self, mode='mean', supports_masking=False, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.supports_masking = supports_masking\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "        \n",
    "    def _sequence_mask(self, seq_emb, actual_seq_length):\n",
    "        batch_size, max_len, emb_dim = seq_emb.shape\n",
    "        max_length_tensor = torch.arange(0,max_len,1).reshape(1,-1).to(self.device) # (T,) -> (1, T)\n",
    "        actual_length_of_seq = actual_seq_length.reshape(-1,1) # (B,) -> (B,1)\n",
    "        mask = max_length_tensor < actual_length_of_seq # (1,T), (B,1) -> (B,T)\n",
    "        mask = mask.unsqueeze(-1) # (B,T,1)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, seq_emb, actual_seq_length):\n",
    "        \"\"\"\n",
    "        Seq_emb: (B,T,E)\n",
    "        actual_seq_length: (B,1) or (B,T) if self.supports_masking=True\"\"\"\n",
    "        if self.supports_masking:\n",
    "            mask = actual_seq_length.float().unsqueeze(-1) # (B, T) -> (B,T,1)\n",
    "            actual_seq_length = torch.sum(mask, dim=1) # (B,T,1) -> (B,1)\n",
    "        else:\n",
    "            mask = self._sequence_mask(seq_emb, actual_seq_length) # (B, T)\n",
    "        self.mask = mask\n",
    "        \n",
    "        if self.mode == 'max':\n",
    "            # apply mask. make masked positions as negative as possible\n",
    "            masked_seq_emb = seq_emb - (1 - mask.float()) * 1e9 # (B,T,C) , (B,T,1) -> (B,T,C)\n",
    "            max_masked_seq_emb= torch.max(masked_seq_emb, dim=1)[0] # (B,T,C) -> (B,C)\n",
    "            return max_masked_seq_emb.unsqueeze(1)\n",
    "        else:\n",
    "            masked_seq_emb = seq_emb * mask # (B,T,C) -> (B,T,1) -> (B,T,C)\n",
    "            if self.mode == 'sum':\n",
    "                sum_masked_seq_emb = torch.sum(masked_seq_emb, dim=1)# (B,T,C) -> (B,C)\n",
    "                return sum_masked_seq_emb.unsqueeze(1)\n",
    "            elif self.mode == 'mean':\n",
    "                sum_masked_seq_emb = torch.sum(masked_seq_emb, dim=1) # (B,T, C) -> (B,C)\n",
    "                mean_masked_seq_emb = sum_masked_seq_emb / actual_seq_length.reshape(-1,1) # (B,C) , (B,1) ->(B,C)\n",
    "                return mean_masked_seq_emb.unsqueeze(1)\n",
    "            else:\n",
    "                raise ValueError(f'mode=\"{self.mode}\" is not supported in {SequentialPoolingLayer.__class__.__name__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad3ed0",
   "metadata": {},
   "source": [
    "# Specify Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9091c60e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8644f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_sparse_categorical_features = ['user_id','movie_id', 'genres','occupation','zip']\n",
    "lst_varlen_sparse_categorical_features = ['hist_movie_id', 'hist_genres']\n",
    "lst_numerical_features = ['age']\n",
    "\n",
    "embedding_dim = 8\n",
    "sparse_categorical_features = [SparseFeat(name=col, \n",
    "                                         vocabulary_size=feature_max_idx[col],\n",
    "                                         embedding_dim=8,) for col in lst_sparse_categorical_features]\n",
    "\n",
    "varlen_sparse_categorical_features =[VarLenSparseFeat(\n",
    "    sparse_feat= SparseFeat(name=col,\n",
    "                            vocabulary_size = feature_max_idx[col.split('hist_')[-1]],\n",
    "                            embedding_dim = embedding_dim),\n",
    "    max_len = 20,\n",
    "    length_name='hist_len',\n",
    "    combiner='mean') for col in lst_varlen_sparse_categorical_features]\n",
    "\n",
    "dense_feats = [DenseFeat(name=col,dimension=1) for col in lst_numerical_features]\n",
    "\n",
    "feature_columns = sparse_categorical_features + varlen_sparse_categorical_features + dense_feats\n",
    "feature_positions = build_feature_positions(feature_columns)\n",
    "\n",
    "torch_dataset = build_pytorch_dataset(data_pd, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6c5bc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_input_dimension(sparse_categorical_features, varlen_sparse_categorical_features, dense_features):\n",
    "#     sparse_fts = [ft for ft in feature_columns if isinstance(ft, SparseFeat) or isinstance(ft, VarLenSparseFeat)]\n",
    "    # compute input dimension\n",
    "    sparse_dims = [ft.embedding_dim for ft in sparse_categorical_features + varlen_sparse_categorical_features]\n",
    "    dense_dims =[ft.dimension for ft in dense_features]\n",
    "    input_dim = sum(sparse_dims + dense_dims)\n",
    "    return input_dim\n",
    "\n",
    "compute_input_dimension(sparse_categorical_features, varlen_sparse_categorical_features, dense_features=dense_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa14d2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57efdfe8",
   "metadata": {},
   "source": [
    "#  Build training and testing data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbb20cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create secondary label for gender\n",
    "train_pd['gender_label'] = train_pd['gender'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "test_pd['gender_label'] = test_pd['gender'].apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d27593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pt = build_pytorch_dataset(train_pd, feature_columns)\n",
    "test_pt = build_pytorch_dataset(test_pd, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7043d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_pt\n",
    "train_y = torch.tensor(train_pd[['label']].values, dtype=torch.long)\n",
    "\n",
    "test_X = test_pt\n",
    "test_y = torch.tensor(test_pd[['label']].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bc0aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X= X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "train_dataset = Dataset(train_X, train_y)\n",
    "test_dataset = Dataset(test_X, test_y)\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                               batch_size= BATCH_SIZE, \n",
    "                                               shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                             batch_size = BATCH_SIZE,\n",
    "                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90a9d12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 47])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b2e7f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model:nn.Module, model_dir: str):\n",
    "    # model_dir must end with .pt extension\n",
    "    torch.save(model.state_dict(), model_dir)\n",
    "\n",
    "def resume_checkpoint(model:nn.Module, model_dir:str, device:torch.device):\n",
    "    state_dict = torch.load(model_dir, map_location=device)\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c004fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\" https://github.com/Bjarten/early-stopping-pytorch/tree/master\n",
    "    \n",
    "    # instantiate EarlyStopping\n",
    "    early_stop = EarlyStopping(patience=5,\n",
    "                                delta=0, # change in performance\n",
    "                                path='checkpoint.pt',\n",
    "                                trace_func=print)\n",
    "       \n",
    "    n_epochs = 10\n",
    "    device = torch.device('cpu')\n",
    "    LEARNING_RATE = 1e-3\n",
    "    optimizer = torch.optim.Adamw(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # basic training log. Can add in things like l1 reg, l2 reg, etc\n",
    "    training_logs = {\n",
    "        'batch_size_train': train_dataloader.batch_size,\n",
    "        'n_samples_train': len(train_dataset),\n",
    "        'steps_per_epoch_train': len(train_dataloader),\n",
    "        'batch_size_test': test_dataloader.batch_size,\n",
    "        'n_samples_test': len(test_dataset),\n",
    "        'steps_per_epoch_test': len(test_dataloader),\n",
    "        'n_epochs': n_epochs,\n",
    "        'device': str(device),\n",
    "        'lr': LEARNING_RATE\n",
    "    }\n",
    "    \n",
    "    for ep in range(n_epochs):\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        model.train() # prep model for training\n",
    "        for idx, (bX, by) in enumerate(train_dataloader):\n",
    "            bX, by = bX.to(device), by.to(device)\n",
    "            out, loss = model(bX, by)\n",
    "            \n",
    "            losses.append(loss.cpu().item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # validatation loss on validation set\n",
    "        model.eval() # # prep model for evaluation\n",
    "        with torch.no_grad():\n",
    "            # prep model for evalaution\n",
    "            for idx, (bX, by) in enumerate(test_dataloader):\n",
    "                bX, by = bX.to(device), by.to(device)\n",
    "                out, loss = model(bX, by)\n",
    "\n",
    "                val_losses.append(loss.cpu().item())\n",
    "            \n",
    "        # calculate training and validation loss\n",
    "        avg_train_loss = np.mean(losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        # early stop using validation loss to prevent overfitting\n",
    "        early_stop(avg_val_loss, model=model)\n",
    "        if early_stop.early_stop:\n",
    "            break\n",
    "            \n",
    "        print(f'[{ep}/{n_epochs}] [Train] avg loss: {avg_train_loss}, avg auc: {train_auc}')\n",
    "        print(f'                   [Validation] avg_loss: {avg_val_loss}, avg auc: {val_auc_roc}')\n",
    "        \n",
    "   # load the last checkpoint with the best model\n",
    "    resume_checkpoint(model, early_stop.path, device=device)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 patience=7,\n",
    "                 verbose= False,\n",
    "                delta=0,\n",
    "                path='checkpoint.pt',\n",
    "                trace_func=print):\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = None\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        # set first val_loss to best_score\n",
    "        if self.val_loss_min is None:\n",
    "#             print('val_loss None')\n",
    "            self.val_loss_min = val_loss\n",
    "            # save model\n",
    "            save_checkpoint(model, self.path)\n",
    "        \n",
    "        # if validation loss is higher than the best score + delta, increment counter and early stop if exceed counter\n",
    "        elif val_loss > self.val_loss_min + self.delta:\n",
    "#             print('valid score higher')\n",
    "            self.counter +=1\n",
    "            self.trace_func(f'EarlyStopping Counter: {self.counter} out of {self.patience}')\n",
    "            \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "        # current validation loss is the best. Reset counter and save the best model\n",
    "        else:\n",
    "#             print('valid score better')\n",
    "            self.val_loss_min = val_loss\n",
    "            self.counter = 0\n",
    "            # save model\n",
    "            save_checkpoint(model, self.path)  \n",
    "            if self.verbose:\n",
    "                self.trace_func(f'Validation loss decreased({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "289f197d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 feature_positions,\n",
    "                 sparse_categorical_features,\n",
    "                 varlen_sparse_categorical_features,\n",
    "                 dense_features,\n",
    "                 l1_reg=0, l2_reg=0,\n",
    "                device='cpu'):\n",
    "        super().__init__()\n",
    "        self.feature_positions = feature_positions\n",
    "        self.embedding_dict = build_embedding_dict(sparse_categorical_features + varlen_sparse_categorical_features, device=device)\n",
    "        self.sparse_categorical_features = sparse_categorical_features\n",
    "        self.varlen_sparse_categorical_features = varlen_sparse_categorical_features\n",
    "        self.dense_features = dense_features\n",
    "        \n",
    "        in_dim = sum([feat.embedding_dim for feat in sparse_categorical_features] \\\n",
    "                               + [feat.embedding_dim for feat in varlen_sparse_categorical_features]\\\n",
    "                              + [feat.dimension for feat in dense_features])\n",
    "        \n",
    "        self.l1_reg = l1_reg\n",
    "        self.l2_reg = l2_reg\n",
    "        self.regularization_weight = []\n",
    "        \n",
    "        self.linear = nn.Sequential(nn.Linear(57,100),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(100,1),\n",
    "                                    nn.Sigmoid())\n",
    "        \n",
    "        weights_for_regularization =filter(lambda x: 'weight' in x[0] and 'bn' not in [0], \n",
    "               self.linear.named_parameters())\n",
    "        \n",
    "        self.add_regularization_weight(weights_for_regularization, l2=self.l2_reg, l1=self.l1_reg)\n",
    "        \n",
    "    def add_regularization_weight(self, weight_list, l1=0.0, l2=0.0):\n",
    "        if isinstance(weight_list, torch.nn.parameter.Parameter):\n",
    "            weight_list = [weight_list]\n",
    "        else:\n",
    "            weight_list = list(weight_list) # for generators\n",
    "        self.regularization_weight.append((weight_list, l1, l2))\n",
    "        \n",
    "    def get_regularization_loss(self):\n",
    "        total_reg_loss = torch.zeros((1,))\n",
    "        for weight_list, l1, l2 in self.regularization_weight:\n",
    "            for w in weight_list:\n",
    "                if isinstance(w, tuple):\n",
    "                    parameter = w[1] # named_parameters\n",
    "                else:\n",
    "                    parameter = w\n",
    "                if l1 > 0:\n",
    "                    total_reg_loss += torch.sum(l1 * torch.abs(parameter))\n",
    "                if l2 > 0:\n",
    "                    total_reg_loss += torch.sum(l2 * parameter * parameter)\n",
    "        return total_reg_loss\n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        \n",
    "        sparse_embeddings = embedding_lookup(X,\n",
    "                                             feature_positions=self.feature_positions,\n",
    "                                             embedding_dict = self.embedding_dict,\n",
    "                                             sparse_feature_columns = self.sparse_categorical_features,\n",
    "                                             to_list=True)\n",
    "        \n",
    "        varlen_sparse_embeddings = varlen_embedding_pooled_lookup(X,\n",
    "                                                                 feature_positions=feature_positions,\n",
    "                                                                 embedding_dict = self.embedding_dict,\n",
    "                                                                 varlen_sparse_feature_columns = self.varlen_sparse_categorical_features,\n",
    "                                                                 to_list=True)\n",
    "        \n",
    "        categorical_features = torch.cat(sparse_embeddings + varlen_sparse_embeddings, dim=2).squeeze()\n",
    "        \n",
    "        dense_input = dense_lookup(X, \n",
    "                                   feature_positions= feature_positions,\n",
    "                                   dense_feat_columns = self.dense_features)\n",
    "        \n",
    "        numerical_features = torch.cat(dense_input, dim=-1)\n",
    "        \n",
    "        # combine the inputs\n",
    "        X = torch.cat([categorical_features, numerical_features], dim=-1).type(torch.float32) \n",
    "        \n",
    "        out = self.linear(X)\n",
    "        loss = torch.nn.functional.binary_cross_entropy(out.view(-1), y.view(-1))\n",
    "        return out, loss\n",
    "    \n",
    "model = Model(feature_positions,\n",
    "              sparse_categorical_features,\n",
    "              varlen_sparse_categorical_features,\n",
    "              dense_features=dense_feats,)\n",
    "out, loss = model(bx.type(torch.float32), by.type(torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f4972d",
   "metadata": {},
   "source": [
    "# Without tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22ae1d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss None\n",
      "[0/10] [Train] avg loss: 0.6720972619950771\n",
      "                   [Validation] avg_loss: 0.6708048433065414\n",
      "valid score better\n",
      "Validation loss decreased(0.669731 --> 0.669731). Saving model...\n",
      "[1/10] [Train] avg loss: 0.6710621789097786\n",
      "                   [Validation] avg_loss: 0.6697314232587814\n",
      "valid score better\n",
      "Validation loss decreased(0.668664 --> 0.668664). Saving model...\n",
      "[2/10] [Train] avg loss: 0.670013889670372\n",
      "                   [Validation] avg_loss: 0.6686636060476303\n",
      "valid score better\n",
      "Validation loss decreased(0.667593 --> 0.667593). Saving model...\n",
      "[3/10] [Train] avg loss: 0.6689565144479275\n",
      "                   [Validation] avg_loss: 0.6675928086042404\n",
      "valid score better\n",
      "Validation loss decreased(0.666523 --> 0.666523). Saving model...\n",
      "[4/10] [Train] avg loss: 0.6679726727306843\n",
      "                   [Validation] avg_loss: 0.6665227711200714\n",
      "valid score better\n",
      "Validation loss decreased(0.665462 --> 0.665462). Saving model...\n",
      "[5/10] [Train] avg loss: 0.6669275462627411\n",
      "                   [Validation] avg_loss: 0.6654621213674545\n",
      "valid score better\n",
      "Validation loss decreased(0.664394 --> 0.664394). Saving model...\n",
      "[6/10] [Train] avg loss: 0.6658204719424248\n",
      "                   [Validation] avg_loss: 0.664394423365593\n",
      "valid score better\n",
      "Validation loss decreased(0.663319 --> 0.663319). Saving model...\n",
      "[7/10] [Train] avg loss: 0.6648391559720039\n",
      "                   [Validation] avg_loss: 0.6633188426494598\n",
      "valid score better\n",
      "Validation loss decreased(0.662247 --> 0.662247). Saving model...\n",
      "[8/10] [Train] avg loss: 0.6638061925768852\n",
      "                   [Validation] avg_loss: 0.6622473746538162\n",
      "valid score better\n",
      "Validation loss decreased(0.661177 --> 0.661177). Saving model...\n",
      "[9/10] [Train] avg loss: 0.662745289504528\n",
      "                   [Validation] avg_loss: 0.6611774861812592\n"
     ]
    }
   ],
   "source": [
    "model = Model(feature_positions,\n",
    "              sparse_categorical_features,\n",
    "              varlen_sparse_categorical_features,\n",
    "              dense_features=dense_feats,)\n",
    "\n",
    "# instantiate EarlyStopping\n",
    "early_stop = EarlyStopping(patience=5,\n",
    "                           delta=0, # change in performance\n",
    "                           path='checkpoint.pt',\n",
    "                           trace_func=print,\n",
    "                           verbose=True)\n",
    "\n",
    "n_epochs = 10\n",
    "device = torch.device('cpu')\n",
    "LEARNING_RATE = 1e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# basic training log. Can add in things like l1 reg, l2 reg, etc\n",
    "training_logs = {\n",
    "    'batch_size_train': train_dataloader.batch_size,\n",
    "    'n_samples_train': len(train_dataset),\n",
    "    'steps_per_epoch_train': len(train_dataloader),\n",
    "    'batch_size_test': test_dataloader.batch_size,\n",
    "    'n_samples_test': len(test_dataset),\n",
    "    'steps_per_epoch_test': len(test_dataloader),\n",
    "    'n_epochs': n_epochs,\n",
    "    'device': str(device),\n",
    "    'lr': LEARNING_RATE\n",
    "}\n",
    "\n",
    "for ep in range(n_epochs):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    model.train() # prep model for training\n",
    "    for idx, (bX, by) in enumerate(train_dataloader):\n",
    "        bX, by = bX.type(torch.float32).to(device), by.type(torch.float32).to(device)\n",
    "        out, loss = model(bX, by)\n",
    "\n",
    "        losses.append(loss.cpu().item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validatation loss on validation set\n",
    "    model.eval() # # prep model for evaluation\n",
    "    with torch.no_grad():\n",
    "        # prep model for evalaution\n",
    "        for idx, (bX, by) in enumerate(test_dataloader):\n",
    "            bX, by = bX.type(torch.float32).to(device), by.type(torch.float32).to(device)\n",
    "            out, loss = model(bX, by)\n",
    "\n",
    "            val_losses.append(loss.cpu().item())\n",
    "\n",
    "    # calculate training and validation loss\n",
    "    avg_train_loss = np.mean(losses)\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "    # early stop using validation loss to prevent overfitting\n",
    "    early_stop(avg_val_loss, model=model)\n",
    "    if early_stop.early_stop:\n",
    "        break\n",
    "\n",
    "    print(f'[{ep}/{n_epochs}] [Train] avg loss: {avg_train_loss}') #,' avg auc: {train_auc}')\n",
    "    print(f'                   [Validation] avg_loss: {avg_val_loss}') #,' avg auc: {val_auc_roc}')\n",
    "\n",
    "# load the last checkpoint with the best model\n",
    "resume_checkpoint(model, early_stop.path, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b507d8",
   "metadata": {},
   "source": [
    "# With TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df5e96b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10] [Train] avg loss: 0.6072225123643875\n",
      "    [Validation] avg_loss: 0.545238196849823\n",
      "Validation loss decreased(0.476654 --> 0.476654). Saving model...\n",
      "[1/10] [Train] avg loss: 0.5131176691502333\n",
      "    [Validation] avg_loss: 0.47665419429540634\n",
      "Validation loss decreased(0.454250 --> 0.454250). Saving model...\n",
      "[2/10] [Train] avg loss: 0.4701571110635996\n",
      "    [Validation] avg_loss: 0.45424970984458923\n",
      "Validation loss decreased(0.448170 --> 0.448170). Saving model...\n",
      "[3/10] [Train] avg loss: 0.45124273374676704\n",
      "    [Validation] avg_loss: 0.44817017018795013\n",
      "Validation loss decreased(0.441148 --> 0.441148). Saving model...\n",
      "[4/10] [Train] avg loss: 0.431005647405982\n",
      "    [Validation] avg_loss: 0.4411483556032181\n",
      "Validation loss decreased(0.433791 --> 0.433791). Saving model...\n",
      "[5/10] [Train] avg loss: 0.4022911563515663\n",
      "    [Validation] avg_loss: 0.4337909370660782\n",
      "Validation loss decreased(0.427999 --> 0.427999). Saving model...\n",
      "[6/10] [Train] avg loss: 0.36380185931921005\n",
      "    [Validation] avg_loss: 0.4279991611838341\n",
      "Validation loss decreased(0.427727 --> 0.427727). Saving model...\n",
      "[7/10] [Train] avg loss: 0.3141019158065319\n",
      "    [Validation] avg_loss: 0.42772702127695084\n",
      "EarlyStopping Counter: 1 out of 5\n",
      "[8/10] [Train] avg loss: 0.26007978804409504\n",
      "    [Validation] avg_loss: 0.43748265504837036\n",
      "EarlyStopping Counter: 2 out of 5\n",
      "[9/10] [Train] avg loss: 0.20926103834062815\n",
      "    [Validation] avg_loss: 0.4599404036998749\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "model = Model(feature_positions,\n",
    "              sparse_categorical_features,\n",
    "              varlen_sparse_categorical_features,\n",
    "              dense_features=dense_feats,)\n",
    "\n",
    "# instantiate EarlyStopping\n",
    "early_stop = EarlyStopping(patience=5,\n",
    "                           delta=0, # change in performance\n",
    "                           path='checkpoint.pt',\n",
    "                           trace_func=print,\n",
    "                           verbose=True)\n",
    "\n",
    "n_epochs = 10\n",
    "device = torch.device('cpu')\n",
    "LEARNING_RATE = 1e-3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# log_dir will contain all the information about whatis to be logged\n",
    "writer = SummaryWriter(log_dir=f\"pytorch_template/\") # tensorboard write\n",
    "\n",
    "# basic training log. Can add in things like l1 reg, l2 reg, etc\n",
    "training_logs = {\n",
    "    'batch_size_train': train_dataloader.batch_size,\n",
    "    'n_samples_train': len(train_dataset),\n",
    "    'steps_per_epoch_train': len(train_dataloader),\n",
    "    'batch_size_test': test_dataloader.batch_size,\n",
    "    'n_samples_test': len(test_dataset),\n",
    "    'steps_per_epoch_test': len(test_dataloader),\n",
    "    'n_epochs': n_epochs,\n",
    "    'device': str(device),\n",
    "    'lr': LEARNING_RATE\n",
    "}\n",
    "\n",
    "\n",
    "writer.add_text('training_logs', str(training_logs), 0)\n",
    "\n",
    "for ep in range(n_epochs):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    model.train() # prep model for training\n",
    "    for idx, (bX, by) in enumerate(train_dataloader):\n",
    "        bX, by = bX.type(torch.float32).to(device), by.type(torch.float32).to(device)\n",
    "        out, loss = model(bX, by)\n",
    "\n",
    "        losses.append(loss.cpu().item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validatation loss on validation set\n",
    "    model.eval() # # prep model for evaluation\n",
    "    with torch.no_grad():\n",
    "        # prep model for evalaution\n",
    "        for idx, (bX, by) in enumerate(test_dataloader):\n",
    "            bX, by = bX.type(torch.float32).to(device), by.type(torch.float32).to(device)\n",
    "            out, loss = model(bX, by)\n",
    "\n",
    "            val_losses.append(loss.cpu().item())\n",
    "\n",
    "    # calculate training and validation loss\n",
    "    avg_train_loss = np.mean(losses)\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    \n",
    "    writer.add_scalar(tag='training/training_loss', scalar_value = avg_train_loss, global_step=ep,)\n",
    "    writer.add_scalar(tag='training/validation_loss', scalar_value = avg_val_loss, global_step=ep,)\n",
    "    \n",
    "    # early stop using validation loss to prevent overfitting\n",
    "    early_stop(avg_val_loss, model=model)\n",
    "    if early_stop.early_stop:\n",
    "        break\n",
    "\n",
    "    print(f'[{ep}/{n_epochs}] [Train] avg loss: {avg_train_loss}') #,' avg auc: {train_auc}')\n",
    "    print(f'    [Validation] avg_loss: {avg_val_loss}') #,' avg auc: {val_auc_roc}')\n",
    "\n",
    "# load the last checkpoint with the best model\n",
    "resume_checkpoint(model, early_stop.path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a4834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.13.0 at http://localhost:8008/ (Press CTRL+C to quit)\n",
      "E1023 20:04:54.646395 6123122688 directory_watcher.py:254] File pytorch_template/events.out.tfevents.1698062572.Kians-MacBook-Air.local.40712.0 updated even though the current file is pytorch_template/events.out.tfevents.1698062588.Kians-MacBook-Air.local.40712.1\n"
     ]
    }
   ],
   "source": [
    "writer.close()\n",
    "\n",
    "!tensorboard --logdir=pytorch_template --port=8008"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
