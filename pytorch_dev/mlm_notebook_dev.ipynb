{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67a560c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0ae08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultLog:\n",
    "    def __init__(self):\n",
    "        self.res = {}\n",
    "        self.DEBUG = False\n",
    "    def __call__(self, key, value):\n",
    "        self.res[key] = value\n",
    "        if self.DEBUG:\n",
    "            print(f'result: {key} with value: {value} logged')\n",
    "        return\n",
    "    def show_results(self):\n",
    "        return self.res\n",
    "    def reset():\n",
    "        self.res = res\n",
    "        print('results resetted.')\n",
    "        return\n",
    "    \n",
    "log = ResultLog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998ea979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 0, 1]), tensor([1, 2, 2]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sample_data': [tensor([7, 0]), tensor([7]), tensor([2, 0, 3])],\n",
       " 'batch_data': tensor([[7, 7, 2],\n",
       "         [0, 8, 0],\n",
       "         [8, 8, 3]]),\n",
       " 'initial_mask': tensor([[False,  True,  True],\n",
       "         [False,  True,  True],\n",
       "         [ True,  True,  True]]),\n",
       " 'omitted_mask': tensor([[False,  True,  True],\n",
       "         [False, False,  True],\n",
       "         [False, False,  True]]),\n",
       " 'unchanged_token': tensor([[False,  True,  True],\n",
       "         [False, False,  True],\n",
       "         [False, False,  True]]),\n",
       " 'random_mask': tensor([[False,  True,  True],\n",
       "         [False, False,  True],\n",
       "         [False, False, False]]),\n",
       " 'combined_mask': tensor([[False, False, False],\n",
       "         [False, False, False],\n",
       "         [False, False, False]]),\n",
       " 'mask_filled_data': tensor([[7, 7, 2],\n",
       "         [0, 8, 2],\n",
       "         [8, 8, 3]]),\n",
       " 'random_tokens': tensor([7, 2, 2]),\n",
       " 'mask_random_filled': tensor([[7, 7, 2],\n",
       "         [0, 8, 2],\n",
       "         [8, 8, 3]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLM(nn.Module):\n",
    "    def __init__(self, vocab_size= 10,\n",
    "                 masking_prob=0.15,\n",
    "                 no_change_prob =0.1,\n",
    "                 randomize_prob = 0.1,\n",
    "                 no_mask_tokens = []):\n",
    "        ## the vocab size in your dictionary. This is inclusive of your [MASK] and [PAD] tokens\n",
    "        self.vocab_size = vocab_size\n",
    "        ## the index in vocabulary to represent the [MASK] token\n",
    "        self.mask_token = vocab_size - 1\n",
    "        ## the index in vocabulary to represent the [PAD] token\n",
    "        self.padding_token = vocab_size - 2\n",
    "        ## probability of masking the input tokens for mlm, aka masking probability\n",
    "        self.masking_prob = masking_prob\n",
    "        ## probability of keeping the masked tokens as randomized tokens\n",
    "        self.randomize_prob = randomize_prob\n",
    "        ## probability of keeping the masked tokens are unchanged tokens\n",
    "        self.no_change_prob = no_change_prob\n",
    "        ## tokens that you do not want to mask. \n",
    "        ## You do not want to mask un-related tokens\n",
    "        ## Sometimes, you also do not want to mask some important tokens\n",
    "        self.no_mask_tokens = no_mask_tokens + [self.mask_token, self.padding_token]\n",
    "        \n",
    "        \n",
    "    def gen_sample_data(self, num_seq = 3):\n",
    "        \"\"\"Generate `num_seq` sentences without any mask or padding tokens\"\"\"\n",
    "        ## you do not want to add in the [MASK] or [PAD] tokens\n",
    "        ## generate sample data of elements from 0 to self.vocab_size-2, of length 5 to 15\n",
    "        single_sentence_generator = lambda : torch.randint(self.vocab_size-2, size=(torch.randint(1,10, size=(1,)),))\n",
    "        return [single_sentence_generator() for _ in range(num_seq)]\n",
    "    \n",
    "    def truncate_data(self, sample_data, max_len=3):\n",
    "        \"\"\"Truncate the tokenized data to max_len, data should be (T,B) \n",
    "        where T is the max_len, and B is the batch_size\n",
    "        \n",
    "        if the length of the sentence is longer than max_len, then we pad the tokens\n",
    "        \"\"\"\n",
    "        ## number of\n",
    "        batch_data = torch.full((max_len, len(sample_data)), self.padding_token)\n",
    "        for i, tokenized_sent in enumerate(sample_data):\n",
    "            seq_len = min(max_len, len(tokenized_sent))\n",
    "            ## `i` is broadcasted. tensor[(1,2), 1] -> tensor[(1,2), (1,1)]. Select elements (1,1) and (2,1)\n",
    "            batch_data[:seq_len, i] = tokenized_sent[:seq_len]\n",
    "        return batch_data\n",
    "    \n",
    "    def mask_tokens(self, batch_data):\n",
    "        \"\"\"Mask the batched data according to self.masking_prob\"\"\"\n",
    "        masking = torch.randn(batch_data.shape) < self.masking_prob\n",
    "#         print()\n",
    "#         masked = batch_data.masked_fill_(masking, self.mask_token) # element-wise multiplication\n",
    "        return masking\n",
    "\n",
    "    def mask_tokens_omit_no_mask(self, batch_data, full_mask):\n",
    "        \"\"\"omit the tokens that should not be masked\"\"\"\n",
    "        for tok in self.no_mask_tokens:\n",
    "            ## full_mask &= batch_data != tok -> inplace operations, more neat\n",
    "            full_mask = full_mask & (batch_data != tok)\n",
    "        return full_mask\n",
    "    \n",
    "    def unchanged_mask_tokens(self, full_mask):\n",
    "        unchanged_token_mask = full_mask & (torch.randn(full_mask.shape) < self.no_change_prob)\n",
    "        return unchanged_token_mask\n",
    "    \n",
    "    def random_mask_tokens(self, full_mask):\n",
    "        random_token_mask = full_mask & (torch.randn(full_mask.shape) < self.randomize_prob)\n",
    "        return random_token_mask\n",
    "    \n",
    "    def combine_mask(self, full_mask, unchanged_token_mask, random_token_mask):\n",
    "        \"\"\"The final set of tokens that are going to be replaced by [MASK] \n",
    "        This is where we make 10% of masking tokens to be unchanged, \n",
    "        10% of masking tokens to be random. Important!!!\n",
    "        \"\"\"\n",
    "        mask = full_mask & ~unchanged_token_mask & ~random_token_mask\n",
    "        ### (masking_omit != random_token_mask) != unchanged_token_mask\n",
    "        ### The above will not work. we want (True & True) = True. \n",
    "        ### The above will set False != True = True.\n",
    "        return mask\n",
    "    \n",
    "    def mask_fill(self, mask, batch_data):\n",
    "        # remember that if you do masked_fill_, it will be an inplace operation\n",
    "        batch_data = batch_data.clone()\n",
    "        mask_filled_data = batch_data.masked_fill_(mask, self.mask_token)\n",
    "        return mask_filled_data\n",
    "    \n",
    "    def fill_random_tokens(self, mask, random_token_mask):\n",
    "        ## returns a tuple of tensors, where the first tuple represents indices in the first dim\n",
    "        ## the second tuple represents indices in the second dimension, and so on if applicable\n",
    "        # # (tensor([0, 0, 1, 2]), tensor([0, 1, 1, 1]))\n",
    "        tuple_indices = torch.nonzero(random_token_mask, as_tuple=True)\n",
    "        print(tuple_indices)\n",
    "        random_tokens = torch.randint(self.vocab_size, size=(len(tuple_indices[0]),))\n",
    "        log('random_tokens', random_tokens)\n",
    "        # you can do indexing with indices isnide a tensor as opposed to a list or tuple\n",
    "        # eg matrix[tensor[1,2], tensor[3,4]] will retrieve matrix[1,3] and matrix[2,4]\n",
    "        mask[tuple_indices] = random_tokens\n",
    "        return mask\n",
    "    \n",
    "    def get_labels(self,batch_data):\n",
    "        return batch_data.clone()\n",
    "    \n",
    "    def format_labels_for_loss(self, y, full_mask):\n",
    "        \"\"\"Assign token [PAD] to all the other locations in the labels. \n",
    "        The labels equal to [PAD] will not be used in the loss.\n",
    "        \"\"\"\n",
    "        return y.masked_fill_(~full_mask, self.padding_token)\n",
    "        \n",
    "    \n",
    "mlm = MLM(vocab_size=10,\n",
    "          masking_prob=0.3)\n",
    "sample_data = mlm.gen_sample_data(num_seq=3); log('sample_data', sample_data)\n",
    "# [tensor([2, 5, 0, 5, 4, 0]), tensor([2]), tensor([1, 0, 1, 1])]\n",
    "\n",
    "batch_data = mlm.truncate_data(sample_data=sample_data); log('batch_data', batch_data)\n",
    "# tensor([[2, 2, 1],\n",
    "#         [5, 8, 0],\n",
    "#         [0, 8, 1]])\n",
    "masking = mlm.mask_tokens(batch_data); log('initial_mask', masking)\n",
    "masking_omit = mlm.mask_tokens_omit_no_mask(batch_data, masking);log('omitted_mask', masking_omit)\n",
    "unchanged_token_mask = mlm.unchanged_mask_tokens(masking_omit); log('unchanged_token', unchanged_token_mask)\n",
    "random_token_mask = mlm.random_mask_tokens(masking_omit); log('random_mask', random_token_mask)\n",
    "mask = mlm.combine_mask(masking_omit,unchanged_token_mask, random_token_mask); log('combined_mask', mask)\n",
    "mask_filled_data = mlm.mask_fill(mask, batch_data); log('mask_filled_data', mask_filled_data)\n",
    "mask_random_filled = mlm.fill_random_tokens(mask_filled_data, random_token_mask); log('mask_random_filled', mask_random_filled)\n",
    "y = mlm.get_labels(batch_data)\n",
    "formatted_y = mlm.format_labels_for_loss(y, masking_omit)\n",
    "log.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94930b",
   "metadata": {},
   "source": [
    "# Loss function need to omit the padding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f86860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `ignore_index` specifies a target value that is ignored\n",
    "loss = nn.CrossEntropyLoss(ignore_index=mlm.padding_token) ## ignore the index for the padding token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
