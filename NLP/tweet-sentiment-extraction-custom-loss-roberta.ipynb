{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3ed869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformersb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('../data/tweet-sentiment-extraction/train.csv').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a37b34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/kianyewngieng/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68 were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import tokenizers\n",
    "ROBERTA_PATH = \"/Users/kianyewngieng/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68\"\n",
    "\n",
    "roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH)\n",
    "\n",
    "ROBERTA_PATH = \"/Users/kianyewngieng/.cache/huggingface/hub/models--roberta-base/snapshots/bc2764f8af2e92b6eb5679868df33e224075ca68\"\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab=f\"{ROBERTA_PATH}/vocab.json\", \n",
    "    merges=f\"{ROBERTA_PATH}/merges.txt\", \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf508e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' I love old school horror movies more, I got Elvira tattooed on my back',\n",
       " 'I love old school horror movies more,')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_s = train.sample(100)\n",
    "tweet = train_s.text.iloc[0]\n",
    "selected_text = train_s.selected_text.iloc[0]\n",
    "sentiment = train_s.sentiment.iloc[0]\n",
    "max_len = 20\n",
    "tweet, selected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc2047e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = ' ' + ' '.join(str(tweet).split())\n",
    "selected_text = ' ' + ' '.join(str(selected_text).split())\n",
    "len_sel_text = len(selected_text)- 1\n",
    "idx_0 = None\n",
    "idx_1 = None\n",
    "for ind in (i for i,e in enumerate(tweet) if e == selected_text[1]):\n",
    "    if ' ' + tweet[ind:ind + len_sel_text] == selected_text:\n",
    "        idx_0 = ind\n",
    "        idx_1 = ind + len_sel_text - 1\n",
    "        break\n",
    "# token_ids(?)\n",
    "# Assign 1 as target for each char in sel_text. This is for the offset\n",
    "# \n",
    "char_targets = [0] * len(tweet)\n",
    "if idx_0 is not None and idx_1 is not None:\n",
    "    for ct in range(idx_0, idx_1+1):\n",
    "        char_targets[ct] = 1\n",
    "    \n",
    "tokenized_tweet = tokenizer.encode(tweet)\n",
    "\n",
    "# vcocab ids\n",
    "input_ids_original = tokenized_tweet.ids\n",
    "# start and end car\n",
    "tweet_offsets = tokenized_tweet.offsets\n",
    "\n",
    "target_ids = []\n",
    "#get id within tweet of words that have target char\n",
    "for i, (offset_0, offset_1) in enumerate(tweet_offsets):\n",
    "    if sum(char_targets[offset_0:offset_1]) > 0:\n",
    "        target_ids.append(i)\n",
    "\n",
    "targets_start = target_ids[0]\n",
    "targets_end = target_ids[-1]\n",
    "\n",
    "sentiment_id = {\"positive\": 1313,\n",
    "               'negative': 2430,\n",
    "                'neutral': 7974}\n",
    "n = len(input_ids_original)\n",
    "sentence = np.arange(n)\n",
    "answer = sentence[targets_start:targets_end+1]\n",
    "\n",
    "def jaccard_array(a, b):\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c) / len(a) + len(b) - len(c))\n",
    "\n",
    "#\n",
    "start_labels = np.zeros(n)\n",
    "for i in range(targets_end +1):\n",
    "    jac = jaccard_array(answer, sentence[i:targets_end +1])\n",
    "    start_labels[i] = jac + jac **2\n",
    "alpha = 0.2\n",
    "start_labels = (1- alpha) * start_labels / start_labels.sum()\n",
    "start_labels[targets_start] += alpha\n",
    "end_labels = np.zeros(n)\n",
    "\n",
    "for i in range(targets_start + 1):\n",
    "    jac = jaccard_array(answer, sentence[i+1: targets_end+1])\n",
    "    end_labels[i] = jac + jac **2\n",
    "    \n",
    "end_labels = (1- alpha) * end_labels / end_labels.sum()\n",
    "end_labels[targets_end]  += alpha\n",
    "\n",
    "start_labels = [0,0,0,0] + list(start_labels) + [0]\n",
    "end_labels = [0,0,0,0] + list(end_labels) + [0]\n",
    "\n",
    "\n",
    "# Input for RoBERTa\n",
    "input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_original + [2]\n",
    "# no token type ids in RoBERTa\n",
    "token_type_ids = [0,0,0,0] + [0] * (len(input_ids_original) + 1)\n",
    "# mask of input without padding\n",
    "mask = [1] * len(token_type_ids)\n",
    "\n",
    "# start and end char ids for each word including new tokens\n",
    "tweet_offsets = [(0,0)] * 4 + tweet_offsets + [(0,0)]\n",
    "# ids within tweet of words that have target char including new tokens\n",
    "targets_start += 4\n",
    "targets_end += 4\n",
    "orig_start = 4\n",
    "orig_end = len(input_ids_original) + 3\n",
    "\n",
    "# Input padding: new mask, token type ids, tweet offsets\n",
    "padding_len = max_len - len(input_ids)\n",
    "if padding_len > 0:\n",
    "    # 1 is padding in roberta!\n",
    "    input_ids = input_ids + ([1] * padding_len)\n",
    "    mask = mask + ([0] * padding_len)\n",
    "    token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "    tweet_offsets = tweet_offsets + ([(0,0)] * padding_len)\n",
    "    start_labels = start_labels + ([0] * padding_len)\n",
    "    end_labels = end_labels + ([0] * padding_len)\n",
    "    \n",
    "targets_select = [0] * len(token_type_ids)\n",
    "for i in range(len(targets_select)):\n",
    "    if i in target_ids:\n",
    "        targets_select[i+4] = 1\n",
    "out = {\n",
    "    'ids': input_ids,\n",
    "    'mask':mask,\n",
    "    'token_type_ids': token_type_ids,\n",
    "    'start_labels':start_labels,\n",
    "    'end_labels': end_labels,\n",
    "    'orig_start': orig_start,\n",
    "    'orig_end': orig_end,\n",
    "    'orig_tweet': tweet,\n",
    "    'orig_selected': selected_text,\n",
    "    'sentiment': sentiment,\n",
    "    'offsets': tweet_offsets,\n",
    "    'targets_select': targets_select\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a748d2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [0,\n",
       "  1313,\n",
       "  2,\n",
       "  2,\n",
       "  939,\n",
       "  657,\n",
       "  793,\n",
       "  334,\n",
       "  8444,\n",
       "  4133,\n",
       "  55,\n",
       "  6,\n",
       "  939,\n",
       "  300,\n",
       "  1615,\n",
       "  705,\n",
       "  3578,\n",
       "  12904,\n",
       "  196,\n",
       "  15,\n",
       "  127,\n",
       "  124,\n",
       "  2],\n",
       " 'mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'start_labels': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.408130081300813,\n",
       "  0.17073170731707318,\n",
       "  0.13658536585365855,\n",
       "  0.10569105691056911,\n",
       "  0.07804878048780489,\n",
       "  0.05365853658536586,\n",
       "  0.032520325203252036,\n",
       "  0.014634146341463415,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0],\n",
       " 'end_labels': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0.8,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0],\n",
       " 'orig_start': 4,\n",
       " 'orig_end': 21,\n",
       " 'orig_tweet': ' I love old school horror movies more, I got Elvira tattooed on my back',\n",
       " 'orig_selected': ' I love old school horror movies more,',\n",
       " 'sentiment': 'positive',\n",
       " 'offsets': [(0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 0),\n",
       "  (0, 2),\n",
       "  (2, 7),\n",
       "  (7, 11),\n",
       "  (11, 18),\n",
       "  (18, 25),\n",
       "  (25, 32),\n",
       "  (32, 37),\n",
       "  (37, 38),\n",
       "  (38, 40),\n",
       "  (40, 44),\n",
       "  (44, 47),\n",
       "  (47, 48),\n",
       "  (48, 51),\n",
       "  (51, 58),\n",
       "  (58, 60),\n",
       "  (60, 63),\n",
       "  (63, 66),\n",
       "  (66, 71),\n",
       "  (0, 0)],\n",
       " 'targets_select': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
